{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ac886b-f210-4951-a9bd-d02f0b3a89b2",
   "metadata": {},
   "source": [
    "# AI-Powered Interviewer Project\n",
    "\n",
    "## Objective:\n",
    "Build an interactive AI-powered interviewer that:\n",
    "- Asks 3–5 technical questions dynamically using a Language Model (LLM)\n",
    "- Adjusts follow-up questions based on user input (branching logic)\n",
    "- Optionally scores and evaluates answers\n",
    "- Provides a final performance summary\n",
    "\n",
    "##  Tools & Technologies:\n",
    "- Python\n",
    "- LangGraph or LangChain\n",
    "- OpenAI / LLaMA3 / Ollama (for LLM)\n",
    "- Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c848f342-e3b9-4d00-b212-91ec33515b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.5.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.8-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.9.0)\n",
      "Collecting packaging>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp311-cp311-win_amd64.whl.metadata (44 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading orjson-3.11.0-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.2.0)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.0)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading langgraph-0.5.4-py3-none-any.whl (143 kB)\n",
      "Downloading langchain_core-0.3.71-py3-none-any.whl (442 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
      "Downloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\n",
      "Downloading langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.11.0-cp311-cp311-win_amd64.whl (129 kB)\n",
      "Downloading ormsgpack-1.10.0-cp311-cp311-win_amd64.whl (121 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: zstandard, typing-extensions, packaging, ormsgpack, orjson, jsonpatch, h11, annotated-types, typing-inspection, pydantic-core, httpcore, pydantic, httpx, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph-prebuilt, langchain, langgraph\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.12\n",
      "    Uninstalling pydantic-1.10.12:\n",
      "      Successfully uninstalled pydantic-1.10.12\n",
      "Successfully installed annotated-types-0.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 langchain-0.3.26 langchain-core-0.3.71 langchain-text-splitters-0.3.8 langgraph-0.5.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.74 langsmith-0.4.8 orjson-3.11.0 ormsgpack-1.10.0 packaging-25.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-extensions-4.14.1 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.11.7 which is incompatible.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 25.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b426751a-5243-454d-b8e6-4d2935adf1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.71)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.8)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.0)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 932.9 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.8/2.5 MB 931.2 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 949.8 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.6/2.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, marshmallow, httpx-sse, dataclasses-json, pydantic-settings, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c7d4da-87e1-4059-b39a-4c01182f7fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.3.71)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.11.7)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from ollama<1.0.0,>=0.5.1->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.11.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.0.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.3.0)\n",
      "Downloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.3.6 ollama-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8b9930-3c07-4a7f-889e-43ff8b778777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8011b2b-7c67-4628-ba5f-e58182ed3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd838d-fe58-4ebe-9df4-e921347374fd",
   "metadata": {},
   "source": [
    "# Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25f764f-af5c-4f97-a88e-c81403665c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (1.26.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a60580-1039-4534-864c-0bcf71144123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a GenAI-powered summarization model using Hugging Face & RAG, improving information retrieval\n",
      "efficiency by 30%.\n",
      "Reduced AWS EC2 costs by 20% using serverless automation with Lambda and EventBridge.\n",
      "Delivered a resume classification prototype with 85%+ accuracy, streamlining role-matching using NLP\n",
      "and supervised models.\n",
      "Web Developer | Digitalize The Globe\n",
      "Aug 2023 - Present\n",
      "Worked as a Web Developer, building and deploying responsive websites with HTML, CSS,\n",
      "JavaScript, and PHP. Managed backend development using MySQL and SQL, and deployed\n",
      "applications on AWS (EC2, S3, Route 53). Built and customized WordPress solutions. Also\n",
      "explored backend automation and POCs using Python, integrated REST APIs, and\n",
      "experimented with FAISS for intelligent content search—laying the foundation for scalable, AI-\n",
      "integrated web applications.\n",
      "AI Engineer Intern Online | AI Variant\n",
      "June 2024 – Feb 2025\n",
      "Worked on developing an end-to-end resume classification system using NLP to automatically map re\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "pdf_path = \"Alwyna Data Science Resume.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "#ectracting text\n",
    "resume_text = \"\"\n",
    "for page in doc:\n",
    "    resume_text += page.get_text()\n",
    "\n",
    "print(resume_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb63d2-017b-414b-a666-4088b148dfe6",
   "metadata": {},
   "source": [
    "# Chunk the Resume Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e264624-035a-48b1-aec6-597c10bcbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "Built a GenAI-powered summarization model using Hugging Face & RAG, improving information retrieval\n",
      "efficiency by 30%.\n",
      "Reduced AWS EC2 costs by 20% using serverless automation with Lambda and EventBridge.\n",
      "Delivered a resume classification prototype with 85%+ accuracy, streamlining role-matching using NLP\n",
      "and supervised models.\n",
      "Web Developer | Digitalize The Globe\n",
      "Aug 2023 - Present\n",
      "Worked as a Web Developer, building and deploying responsive websites with HTML, CSS,\n",
      "\n",
      "--- Chunk 2 ---\n",
      "efficiency by 30%.\n",
      "Reduced AWS EC2 costs by 20% using serverless automation with Lambda and EventBridge.\n",
      "Delivered a resume classification prototype with 85%+ accuracy, streamlining role-matching using NLP\n",
      "and supervised models.\n",
      "Web Developer | Digitalize The Globe\n",
      "Aug 2023 - Present\n",
      "Worked as a Web Developer, building and deploying responsive websites with HTML, CSS,\n",
      "JavaScript, and PHP. Managed backend development using MySQL and SQL, and deployed\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Delivered a resume classification prototype with 85%+ accuracy, streamlining role-matching using NLP\n",
      "and supervised models.\n",
      "Web Developer | Digitalize The Globe\n",
      "Aug 2023 - Present\n",
      "Worked as a Web Developer, building and deploying responsive websites with HTML, CSS,\n",
      "JavaScript, and PHP. Managed backend development using MySQL and SQL, and deployed\n",
      "applications on AWS (EC2, S3, Route 53). Built and customized WordPress solutions. Also\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=500\n",
    ")\n",
    "\n",
    "resume_chunks = text_splitter.split_text(resume_text)\n",
    "\n",
    "for i, chunk in enumerate(resume_chunks[:3]):\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7439d8-7384-435e-8604-a4e86535a4f9",
   "metadata": {},
   "source": [
    "# Converting Chunks to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9530f78b-334f-4ef7-ad51-6cbe4bd13683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "resume_embeddings = embedding_model.embed_documents(resume_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889cd1b-5388-4bc5-a04e-4f09f8b3f15d",
   "metadata": {},
   "source": [
    "# Storing in Vector Store (FAISS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3334dba1-ba4a-4453-9206-05b4ea2df9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\manisha chandanshiv\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.9 MB 3.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 4.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.4/14.9 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.0/14.9 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.8/14.9 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.1/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.2/14.9 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.0/14.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.3/14.9 MB 5.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.3/14.9 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.8/14.9 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.9/14.9 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41fdb1be-3efe-4288-b306-6ec8099ebdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "#chunks to document obejcts \n",
    "documents = [Document(page_content=chunk) for chunk in resume_chunks]\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "vectorstore.save_local(\"resume_vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c5c4-ef24-4beb-a7b5-db6c5399e3b6",
   "metadata": {},
   "source": [
    "# Retrieval + Interview Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d199c84-d1f7-4ab6-babd-0cfa875ebe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.load_local(\n",
    "                                \"resume_vectorstore\", \n",
    "                                embeddings=embedding_model,\n",
    "                                allow_dangerous_deserialization=True\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28f03161-d8ea-4076-a1cb-6295e19d289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37253c-ab41-4378-862c-aed8edb6b627",
   "metadata": {},
   "source": [
    "# Creating a QA Chain (RAG-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5825425b-b87a-4d5c-8951-a694c8e1975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c21227-4c47-461b-82b8-922738575aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Interviewer: According to the provided context, it appears that you (Alwyna Chandanshive) designed an LLM-based chatbot using LangChain + OpenAI API, improving query resolution accuracy by 2%. This suggests that you have experience working with LLMs and LangChain.\n"
     ]
    }
   ],
   "source": [
    "question = \"What experience do you have with LLMs or Langchain?\"\n",
    "response = qa_chain.invoke({\"query\":question})\n",
    "print(\"AI Interviewer:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216e2a9-17d1-4d4c-b1dd-7f690d8aa032",
   "metadata": {},
   "source": [
    "# LLM Generated Questions (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcac7d82-cff9-4db6-8d79-efbe810ca72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic for your interview (e.g., Python, LLMs, Machine Learning): python\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic for your interview (e.g., Python, LLMs, Machine Learning):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63268eec-52b8-488d-bddf-59da52845e82",
   "metadata": {},
   "source": [
    "# Creating a prompt to generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17d9573e-61ef-478b-be70-567a41b3c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "question_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert technical interviewer.\n",
    "Generate 3 to 5 technical interview qusetions on the topic: {topic}.\n",
    "Start each question with a number. Keep the difficulty moderate to advanced.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f4b74cd-57f7-4e86-b856-41036e2ccb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question:\n",
      "\n",
      "Here are 4 Python technical interview questions, ranging from moderate to advanced difficulty:\n",
      "\n",
      "**1.** Given a list of integers `numbers` and an integer `target`, write a function that returns the indices of two elements in the list that add up to the target value.\n",
      "\n",
      "Example: If `numbers = [2, 7, 11, 15]` and `target = 9`, the function should return `[0, 1]` because `numbers[0] + numbers[1] = 9`.\n",
      "\n",
      "**2.** Implement a Python function that takes a string as input and returns the first unique character in the string. If no unique characters are found, return an empty string.\n",
      "\n",
      "Example: If the input is `\"banana\"`, the function should return `\"a\"` because `a` is the only unique character.\n",
      "\n",
      "**3.** Write a Python function that takes a 2D list of integers as input and returns the maximum sum of a contiguous subarray within the 2D array.\n",
      "\n",
      "Example: If the input is `[[1, 2, -1, 4], [-3, -1, 0, 4], [2, 1, -5, 4]]`, the function should return `10` because the maximum sum of a contiguous subarray is `[4, 4] = 8 + 2`.\n",
      "\n",
      "**4.** Implement a Python function that takes a list of strings as input and returns the longest common prefix among all strings in the list.\n",
      "\n",
      "Example: If the input is `[\"hello\", \"hello world\", \"helloabc\"]`, the function should return `\"hello\"` because it's the longest common prefix.\n",
      "\n",
      "Let me know if you'd like me to clarify or provide hints for any of these questions!\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = question_prompt.format(topic=topic)\n",
    "\n",
    "questions_text = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"Generated Question:\\n\")\n",
    "print(questions_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f5503b3-09fb-43d7-a579-824484ce7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final List of Question:\n"
     ]
    }
   ],
   "source": [
    "question_list = [q.strip() for q in questions_text.split(\"\\n\") if q.strip() and q[0].isdigit()]\n",
    "\n",
    "print(\"\\nFinal List of Question:\")\n",
    "for q in question_list:\n",
    "    prnit(\"-\",q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "933dd230-7ef8-43b5-85d2-1a80dc208555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final List of Questions:\n",
      "1. Given a list of integers `numbers` and an integer `target`, write a function that returns the indices of two elements in the list that add up to the target value.\n",
      "2. Implement a Python function that takes a string as input and returns the first unique character in the string. If no unique characters are found, return an empty string.\n",
      "3. Write a Python function that takes a 2D list of integers as input and returns the maximum sum of a contiguous subarray within the 2D array.\n",
      "4. Implement a Python function that takes a list of strings as input and returns the longest common prefix among all strings in the list.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "question_list = re.findall(r\"\\*\\*\\d+\\.\\*\\*\\s+(.*)\", questions_text)\n",
    "\n",
    "print(\"\\n Final List of Questions:\")\n",
    "for i, q in enumerate(question_list, start=1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174d4ba-6026-4991-9406-bc057562b6fb",
   "metadata": {},
   "source": [
    "# Creating Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9292d27-c193-471a-9750-1335e3c48898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "evaluation_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a technical interviewer.\n",
    "\n",
    "Evaluate the candidate\"s Response to the following question.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Give Feedback and rate the response out of 5 on:\n",
    "-Accuracy\n",
    "-Clarity\n",
    "-Depth of Explination\n",
    "\n",
    "Return your answer in this format:\n",
    "Accuracy: #\n",
    "Clarity: #\n",
    "Depth: #\n",
    "Feedback: <short feedback>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f891b1c-3214-459c-8376-fbf27452e38d",
   "metadata": {},
   "source": [
    "# Creating the Interview Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df1a2109-17ad-476b-bdbe-480afd7f30e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question1: Given a list of integers `numbers` and an integer `target`, write a function that returns the indices of two elements in the list that add up to the target value.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Answer:  num = 2, i = 0 complement = 9 - 2 = 7 7 is not in hash_map. Add 2 and its index 0 to the map: hash_map = {2: 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Interviewer Feedback:\n",
      "I'll evaluate the candidate's response based on the provided code snippet and provide my rating.\n",
      "\n",
      "**Accuracy:** 2/5\n",
      "The code doesn't seem to be complete, as it only shows the initialization of a variable `num` and the calculation of its complement. It doesn't demonstrate how to solve the original problem of finding the indices of two elements that add up to the target value. \n",
      "\n",
      "**Clarity:** 4/5\n",
      "The code snippet is easy to understand, but it's not clear what the purpose of initializing `num` and calculating its complement is. A better explanation would be helpful.\n",
      "\n",
      "**Depth of Explanation:** 1/5\n",
      "As mentioned earlier, the code snippet lacks a comprehensive explanation of how to solve the problem. The candidate seems to have missed the point of the question.\n",
      "\n",
      "**Feedback:** While the candidate has demonstrated an understanding of basic programming concepts, they seem to have misinterpreted the problem. To improve, they should focus on providing more complete and relevant code snippets that accurately address the given questions.\n",
      "\n",
      "Overall Rating: 7/15\n",
      "\n",
      " Question2: Implement a Python function that takes a string as input and returns the first unique character in the string. If no unique characters are found, return an empty string.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Answer:  from collections import Counter  def find_first_unique_char(s: str) -> str:     \"\"\"     Finds the first unique character in a string.      Args:         s: The input string.      Returns:         The first unique character in the string, or an empty string if         no unique characters are found.     \"\"\"     char_counts = Counter(s)      for char in s:         if char_counts[char] == 1:             return char     return \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Interviewer Feedback:\n",
      "Here's my evaluation:\n",
      "\n",
      "Accuracy: 5\n",
      "The candidate's implementation is accurate. It correctly identifies the first unique character in the input string, or returns an empty string if no unique characters are found.\n",
      "\n",
      "Clarity: 4\n",
      "The code is well-organized and easy to read. The variable name `char_counts` is descriptive, and the use of a docstring provides clear documentation for the function's purpose and parameters. However, the function could be improved with more informative variable names (e.g., `unique_chars` instead of `char_counts`) and clearer variable usage.\n",
      "\n",
      "Depth: 3\n",
      "The candidate demonstrates a good understanding of Python programming concepts and uses a relevant library (collections) to count character frequencies. However, their explanation is limited to a brief description of the function's purpose and parameters, with no additional insights or justifications for their approach.\n",
      "\n",
      "Feedback: The code is mostly correct and well-organized, but could benefit from more descriptive variable names and a clearer explanation of the problem-solving process.\n",
      "\n",
      " Question3: Write a Python function that takes a 2D list of integers as input and returns the maximum sum of a contiguous subarray within the 2D array.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Answer:  def max_sum_contiguous_subarray_2d(matrix):     \"\"\"     Calculates the maximum sum of a contiguous subarray within a 2D array.      Args:         matrix: A 2D list of integers.      Returns:         The maximum sum of a contiguous subarray.     \"\"\"     rows = len(matrix)     if rows == 0:         return 0     cols = len(matrix[0])     if cols == 0:         return 0      max_sum = float('-inf')      for left_col in range(cols):         # Initialize a temporary 1D array to store column sums for the current range         temp = [0] * rows         for right_col in range(left_col, cols):             # Accumulate column sums for the current range of columns             for i in range(rows):                 temp[i] += matrix[i][right_col]              # Apply Kadane's algorithm to the temporary 1D array             current_max_sum_1d = 0             current_sum_1d = 0             for x in temp:                 current_sum_1d += x                 if current_sum_1d > current_max_sum_1d:                     current_max_sum_1d = current_sum_1d                 if current_sum_1d < 0:                     current_sum_1d = 0                          # Handle the case where all numbers in temp are negative             if current_max_sum_1d == 0 and any(t < 0 for t in temp):                 current_max_sum_1d = max(temp)               max_sum = max(max_sum, current_max_sum_1d)      return max_sum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Interviewer Feedback:\n",
      "Here's my evaluation:\n",
      "\n",
      "Accuracy: 5\n",
      "The candidate's solution is accurate. It correctly calculates the maximum sum of a contiguous subarray within a 2D array.\n",
      "\n",
      "Clarity: 4\n",
      "The code is well-organized and easy to follow, but there are some minor issues with clarity. For example, the variable names `left_col` and `right_col` could be more descriptive. Additionally, the comment \"Handles the case where all numbers in temp are negative\" could be expanded upon.\n",
      "\n",
      "Depth of Explanation: 3\n",
      "The candidate provides a good explanation of their solution, but it's not overly detailed or concise. They could have elaborated on why they chose to use Kadane's algorithm and how it applies to this problem.\n",
      "\n",
      "Feedback:\n",
      "Overall, the candidate's response is strong. The code is accurate and well-organized, and they provide a clear explanation of their approach. However, there are some minor areas for improvement in terms of clarity and depth of explanation.\n",
      "\n",
      " Question4: Implement a Python function that takes a list of strings as input and returns the longest common prefix among all strings in the list.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Answer:  I dont know the answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Interviewer Feedback:\n",
      "Here's my evaluation:\n",
      "\n",
      "Accuracy: 0/5\n",
      "The candidate did not provide an accurate answer, as they simply stated \"I don't know the answer\".\n",
      "\n",
      "Clarity: 1/5\n",
      "While the candidate's response was clear in that it was a direct and honest statement, it lacked any further elaboration or explanation.\n",
      "\n",
      "Depth of Explanation: 0/5\n",
      "The candidate did not provide any additional information or insight into their thought process or potential approaches to solving the problem. Their answer was simply a statement of uncertainty.\n",
      "\n",
      "Feedback:\n",
      "While it's okay to not know an answer off the top of your head, it's important to show willingness to learn and explore possible solutions. In this case, the candidate could have shown more initiative by asking clarifying questions or attempting to outline a potential approach.\n"
     ]
    }
   ],
   "source": [
    "interview_results = []\n",
    "\n",
    "for i, question in enumerate(question_list, 1):\n",
    "    print(f\"\\n Question{i}: {question}\")\n",
    "    answer = input(\"Your Answer: \")\n",
    "\n",
    "    eval_prompt = evaluation_prompt.format(question=question, answer=answer)\n",
    "\n",
    "    evaluation = llm.invoke(eval_prompt)\n",
    "\n",
    "    interview_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"evaluation\": evaluation\n",
    "    })\n",
    "\n",
    "    print(\"\\n Interviewer Feedback:\")\n",
    "    print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cda0a-b6fb-4048-99b0-26db513035e1",
   "metadata": {},
   "source": [
    "# Calculating Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05585294-20b9-4c9f-8e96-5c8b5b9ce329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Average Scores:\n",
      "Accuracy: 5.00\n",
      "Clarity: 4.00\n",
      "Depth: 3.00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "total_accuracy = 0 \n",
    "total_clarity = 0 \n",
    "total_depth = 0 \n",
    "count = 0\n",
    "\n",
    "for result in interview_results:\n",
    "    eval_text = result[\"evaluation\"]\n",
    "    scores = re.findall(r'\\b(?:Accuracy|Clarity|Depth):\\s*(\\d)', eval_text)\n",
    "    if len(scores) == 3:\n",
    "        total_accuracy += int(scores[0])\n",
    "        total_clarity += int(scores[1])\n",
    "        total_depth += int(scores[2])\n",
    "        count += 1\n",
    "\n",
    "if count > 0:\n",
    "    avg_accuracy = total_accuracy / count\n",
    "    avg_clarity = total_clarity / count\n",
    "    avg_depth = total_depth / count\n",
    "\n",
    "    print(\"\\nFinal Average Scores:\")\n",
    "    print(f\"Accuracy: {avg_accuracy:.2f}\")\n",
    "    print(f\"Clarity: {avg_clarity:.2f}\")\n",
    "    print(f\"Depth: {avg_depth:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No scores found to summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d6367-3c23-4ab6-abc8-6b87cea50d25",
   "metadata": {},
   "source": [
    "# Final AI Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2038387f-0d73-4c56-bd71-3ad0f4357a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 Final Summary by AI Interviewer:\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The candidate demonstrated varying levels of strength in their technical question responses. On the positive side, they showed accuracy and clarity in some questions, such as identifying the first unique character in a string or calculating the maximum sum of a contiguous subarray within a 2D array. However, there were areas for improvement, including:\n",
      "\n",
      "* Lack of completeness and relevance in their code snippets\n",
      "* Limited depth of explanation, often failing to provide additional insights into their thought process or problem-solving approaches\n",
      "* Opportunities to improve clarity by using more descriptive variable names and expanding on explanations\n",
      "\n",
      "**Professional Closing Remark:**\n",
      "\n",
      "To excel in technical interviews, it's essential to not only demonstrate a strong foundation in programming concepts but also showcase your ability to communicate complex ideas clearly and concisely. While you showed promise in certain areas, there are clear opportunities for growth and development. I encourage you to focus on providing more comprehensive code snippets, elaborating on your thought processes, and using descriptive variable names to improve the clarity of your responses. With dedication and practice, I'm confident that you'll become a stronger candidate in future technical interviews.\n"
     ]
    }
   ],
   "source": [
    "summary_prompt = \"\"\"\n",
    "You are an AI interviewer.\n",
    "\n",
    "Here is the candidate's performance across a few technical questions. Each includes your evaluation.\n",
    "\n",
    "{}\n",
    "\n",
    "Based on these evaluations, give:\n",
    "- A summary of strengths and areas for improvement\n",
    "- A professional closing remark\n",
    "\n",
    "Keep it concise but helpful.\n",
    "\"\"\"\n",
    "\n",
    "# Combine all evaluations into one input\n",
    "combined_feedback = \"\\n\\n\".join([r[\"evaluation\"] for r in interview_results])\n",
    "\n",
    "# Format prompt and ask LLM\n",
    "final_feedback = llm.invoke(summary_prompt.format(combined_feedback))\n",
    "\n",
    "print(\"\\n🧾 Final Summary by AI Interviewer:\\n\")\n",
    "print(final_feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128fe1d-f6d5-4144-8599-1198ba8f394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbebeb-03db-4b70-9b1a-efa0c6dd8a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5934e-a835-4e8a-8fb1-1853f0a6e08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
